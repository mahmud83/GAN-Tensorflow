import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("./mnist/data/", one_hot=True)

# Hyper parameter
total_epoch = 100
batch_size = 100
n_hidden = 256
image_size = 28 * 28
n_noise = 128
n_class = 10

# neural network model
X = tf.placeholder(tf.float32, [None, image_size])
# It is used to add information about the noise and the actual image to the corresponding number.
Y = tf.placeholder(tf.float32, [None, n_class])
Z = tf.placeholder(tf.float32, [None, n_noise])

def generator(noise, labels) :
    with tf.variable_scope('generator') :
        # Adding labels to the noise value
        inputs = tf.concat([noise, labels], axis = 1)

        hidden = tf.layers.dense(inputs=inputs, units=n_hidden, activation=tf.nn.relu)

        # generate image
        output = tf.layers.dense(inputs=hidden, units=image_size, activation=tf.nn.sigmoid)


        return output

def discriminator(inputs, labels, reuse=None) :
    with tf.variable_scope('discriminator') as scope :
        # In order to make the parameters of the model that discriminates the actual image from the image generated by the noise the same,
        # Allows reuse of previously used variables.
        if reuse :
            scope.reuse_variables()

        inputs = tf.concat([inputs, labels], axis = 1)

        hidden = tf.layers.dense(inputs=inputs, units=n_hidden, activation=tf.nn.relu)

        output = tf.layers.dense(inputs=hidden, units=1, activation=None)

    return output

def get_noise(batch_size, n_noise) :
    return np.random.uniform(-1., 1., size=[batch_size, n_noise])

# Add Y which is labels information to the generation model & discriminator model
# Create an image corresponding to the labels information
G = generator(noise=Z, labels=Y)
D_real = discriminator(inputs=X, labels=Y)
D_gene = discriminator(inputs=G, labels=Y, reuse=True)

# http://bamos.github.io/2016/08/09/deep-completion/
# The D_real value that determines the true image is close to 1
# The D_gene value that determines a fake image is a loss function that is close to zero
loss_D_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, labels=tf.ones_like(D_real)))
loss_D_gene = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene, labels=tf.zeros_like(D_gene)))

# After adding loss_D_real and loss_D_gene, optimize it to minimize this value
loss_D = loss_D_real + loss_D_gene

# It is a loss function that makes D_gene as close to 1 as possible, so that the generator network is learned to make the fake image close to real.
loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene, labels=tf.ones_like(D_gene)))

